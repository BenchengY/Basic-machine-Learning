---
layout: post
title:  "统计学习方法 - 决策树之学习和剪枝"
date:   2017-06-01 16:23:00 +0800
categories: machine_learning_basis posts
---

本章是李航的《统计学习方法》第5章决策树的阅读笔记，另外加上用python实现决策树的实验。分为三部分，1是对决策树背景做一个概述，2是决策树实现的各类算法，3是使用sklearn的desicionTree接口。

以下将对决策树生成算法和剪枝算法做一个介绍。

## 生成算法

决策树生成算法主要有**ID3**、**C4.5**以及**CART**。其中**CART**算法又有回归树和分类树两种。

**ID3**算法使用**信息增益**作为选择特征的标准，将原训练数据集一再划分。

**C4.5**算法使用**信息增益比**作为选择特征的标准，将原训练数据集一再划分。

下面介绍**CART**算法。

#### CART回归树

对于连续的数据，使用回归树预测结果。

目的就是将数据集划分成方差（最小二乘误差）尽可能小的数据子集。

整个伪代码如下：

```
输入：训练数据集和停止条件
输出：CART回归树
for feature in features:
    for value in feature_values:
        根据value将原数据集划分成R1和R2两个数据集；
        计算结果平均值c1=avg(y(R1)), c2=avg(y(R2))；
        计算R1和R2的方差之和；
选择方差之和最小的feature和value作为划分标准；
继续对两个子集进行上述步骤，直至满足停止条件。
```

例子如下，训练集：

|       | $x^{1}$ | $x^{2}$ | $y$  |
| ----- | ------- | ------- | ---- |
| $x_1$ | 2.3     | 2.5     | 5.0  |
| $x_2$ | 3.9     | 3.0     | 9.0  |
| $x_3$ | 1.0     | 1.3     | 2.0  |
| $x_4$ | 1.2     | 5.6     | 4.5  |

$j=1$:

$s=2.3$，划分成$R_1=$ {$ x \mid x^{j}\leq 2.3$}以及$R_2=$ {$ x \mid x^{j}>  2.3 $}，即$x_1, x_3, x_4$ 和$x_2$。得$c_1=\frac{y_1+y_3+y_4}{3}\approx3.83$，$c_2=\frac{y_4}{1}=9.0$，$square_{error1}=1.3689+3.3489+0.4489=5.1667$，$square_{error2}=0$，$error=5.1667$

$s=3.9$，划分成$R_1=$ {$ x \mid x^{j}\leq 3.9$}以及$R_2=$ {$ x \mid x^{j}>  3.9$}，即$ x_1, x_2, x_3, x_4$。得$c_1=\frac{y_1+y_2+y_3+y_4}{4}=5.125$，$c_2=0$，$square_{error1}=0.015625+15.015625+9.765625+0.390625=25.1875$，$square_{error2}=0$，$error=25.1875$

$s=1.0$，划分成$R_1=$ {$ x \mid x^{j}\leq 1.0$}以及$R_2=$ {$ x \mid x^{j}> 1.0$}，即$ x_3$ 和$x_1, x_2,x_4$。得$c_1=\frac{y_3}{1}=2.0$，$c_2=\frac{y_1+y_2+y_4}{3}\approx6.17$，$square_{error1}=1.3689+8.0089+2.7889=12.1667$，$square_{error2}=0$，$error=12.1667$

$s=1.2$，划分成$R_1=$ {$ x \mid x^{j}\leq 1.2$}以及$R_2=$ {$ x \mid x^{j}>  1.2 $}，即$x_1, x_2$ 和$x_3, x_4$。得$c_1=\frac{y_2+y_1}{2}=7$，$c_2=\frac{y_3+y_4}{2}=3.25$，$square_{error1}=4+4=8$，$square_{error2}=1.5625+1.5625=3.125$，$error=11.125$

$j=2$:

$s=2.5$，划分成$R_1=$ {$ x \mid x^{j}\leq 2.5$}以及$R_2=$ {$ x \mid x^{j}>  2.5$}，即$x_1, x_3$ 和$x_2, x_4$。得$c_1=\frac{y_1+y_3}{2}=3.5$，$c_2=\frac{y_2+y_4}{2}=6.75$，$square_{error1}=2.25+2.25=4.5$，$square_{error2}=5.0625+5.0625=10.125$，$error=14.625$

$s=3.0$，划分成$R_1=$ {$ x \mid x^{j}\leq 3.0$}以及$R_2=$ {$ x \mid x^{j}>  3.0 $}，即$x_1,x_2, x_3$ 和$ x_4$。得$c_1=\frac{y_1+y_2+y_3}{3}\approx5.3$，$c_2=\frac{y_4}{1}=4.5$，$square_{error1}=0.09+7.29+10.89=18.27$，$square_{error2}=0$，$error=18.27$

$s=1.3$，划分成$R_1=$ {$ x \mid x^{j}\leq 1.3$}以及$R_2=$ {$ x \mid x^{j}>  1.3 $}，即$ x_3$ 和$x_1, x_2,x_4$。得$c_1=\frac{y_3}{1}=2.0$，$c_2=\frac{y_1+y_2+y_4}{3}\approx6.17$，$square_{error1}=0$，$square_{error2}=1.3689+8.0089+2.7789=12.1667$，$error=12.1667$

$s=5.6$，划分成$R_1=$ {$ x \mid x^{j}\leq 5.6$}以及$R_2=$ {$ x \mid x^{j}>  5.6$}，即$ x_1, x_2, x_3, x_4$。得$c_1=\frac{y_1+y_2+y_3+y_4}{4}=5.125$，$c_2=0$，$square_{error1}=0.015625+15.015625+9.765625+0.390625=25.1875$，$square_{error2}=0$，$error=25.187$

于是当$j=1, s=2.3$时，划分得到的子集方差最小，所以划分成$x_1, x_3, x_4$ 和$x_2$。接着再对这两个子集分别重复以上方法进行划分即可，这里不再赘述。最终的停止条件人为设定，如误差的阈值等。

#### CART分类树

对于离散的数据，使用分类树分类数据

使用基尼指数(Gini)选择特征。

**定义1(基尼指数)：** 假设有$k$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：


$$
Gini(p)=\sum^{K}_{k=1}p_k(1-p_k)=1-\sum^{K}_{k=1}p_k^2
$$

在特征A的划分下，集合D的基尼指数为：


$$
Gini(D, A)=\frac{D_1}{D}Gini(D_1)+\frac{D_2}{D}Gini(D_2)
$$

$$
Gini(D_i)=1-\sum^{K}_{k=1}(\frac{\mid C_k \mid}{\mid D_i \mid})^2
$$

其中，$C_k$表示数据集$D_i$中属于$k$类的样本集合。

## 剪枝算法

如果决策树结构很复杂，可能对训练数据集的分类效果特别好，但是对新来的数据分类不准确，即过拟合。剪枝通过剪去一部分结点来简化树的结构从而方式过拟合。

#### 使用熵提取特征的决策树剪枝算法

剪枝的必要条件是，剪完之后的树$T_A$的损失函数值比原树$T_B$的损失函数值小，即$C_\alpha(T_A) \leq C_\alpha(T_B)$。

设有树$T$，共有$\mid T \mid$个叶结点，$t$为树$T$其中一个叶结点，该叶结点上有$N_t$个样本点，其中$k$类的样本点有$N_tk$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha \geqslant 0$，损失函数如下计算：


$$
C_\alpha(T)=C(T)+\alpha \mid T \mid =\sum^{\mid T \mid}_{t=1}N_tH_t(T)+\alpha\mid T \mid
$$


其中，叶结点$t$上的经验熵为：


$$
H_t(T)=-\sum_{k} \frac{N_{tk}}{N_t}log \frac{N_{tk}}{N_t}
$$


损失函数的第一项就是所有叶结点的经验熵之和，意味着整棵树的不确定性，第一项越大，不确定性越大。第二项表示树的复杂程度，$\mid T \mid$越大则树越复杂。$\alpha$就是用来权衡这两项的，$\alpha$越大但要使得第二项小的话，则$\mid T \mid$就得更小一些，于是树越简单。

所以剪枝的目的就是让损失函数越小，让树在结构不是太复杂的情况下，可以达到较小的不确定性。

#### CART决策树剪枝算法

与上面的剪枝算法不同，这个算法在确定了最优树的同时，超参数$\alpha$也一并确定了。

该剪枝算法从原决策树的底端按照一定的标准不断剪枝，直到根结点，形成一个子树序列${T_0,…,T_n}$，然后用交叉验证选出最优子树。

设生成的决策树为$T_0$，其任意内部结点$t$，以$t$为单结点树的损失函数为


$$
C_\alpha(t)=C(t)+\alpha \mid t \mid=C(t)+\alpha
$$


以$t$为根结点的子树$T_t$的损失函数为


$$
C_\alpha(T_t)=C(T_t)+\alpha\mid T_t \mid
$$


通过计算可得，当$\alpha=\frac{C(t)-C(T_t)}{\mid T_t \mid -1}$时，单结点树和子树$T_t$的损失函数值相同，如果$\alpha$再增大，单结点的损失函数将小于子树$T_t$，所以当$\alpha$大于这个值的时候，单结点树结点比子树少，应该对$T_t$进行剪枝变成单结点树。

令$g(t)=\frac{C(t)-C(T_t)}{\mid T_t \mid -1}$，可以把这个看成是一个阈值，当$\alpha$慢慢上升到$g(t_1)$时，以$t_1$为根结点的子树$T_{t_1}$就会被砍掉，再慢慢上升到$g(t_2)$时，以$t_2$为根结点的子树$T_{t_2}$就会被砍掉......就这样会形成子树序列${T_0,…,T_n}$。

接着用交叉验证选出最优子树。

## Reference

李航 《统计学习方法》第5章 决策树











