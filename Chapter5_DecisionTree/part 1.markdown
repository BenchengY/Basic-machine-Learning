---
layout: post
title:  "统计学习方法 - 决策树之特征选择"
date:   2017-05-22 19:01:00 +0800
categories: machine_learning_basis posts
---

本章是李航的《统计学习方法》第5章决策树的阅读笔记，另外加上用python实现决策树的实验。分为三部分，1是对决策树背景做一个概述，2是决策树实现的各类算法，3是使用sklearn的desicionTree接口。

决策树的实质就是**条件概率分布**——在上一结点**X**的条件下进入下一个结点**Y**的条件概率$P(Y \mid X)$。



## 特征的选择

如果按照特征分类的结果和随机分类的结果一样，不确定性非常大， 那么这个特征是没有用的。所以特征的引入必须是能够降低分类不确定性的。衡量特征对分类的不确定性的下降标准称为**信息增益**。

**定义1（信息增益）：**引入特征X的信息而使得类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益$g(D, A)$的数学定义为，集合D的经验熵H(D)与在特征A给定下D的经验条件熵$H(D \mid A)$之差，即



$$
g(D, A)=H(D)-H(D|A)
$$


注：关于熵和条件熵的定义，在文末参考处提出。现在只需要明白，**熵是表示随机变量不确定性的度量**。熵越大，不确定性越大。另外，当熵和条件熵的概率是由**数据估计**（极大似然估计）得到的时候，被称作经验熵和经验条件熵。

通俗的讲，D表示明天是晴天，特征A表示明天气温很高，于是在知道明天气温很高的情况下，明天是晴天的可能性提升，不确定性下降，这个不确定性下降的程度就表示为信息增益。

显然，对于数据集D而言，信息增益依赖于特征，信息增益越大的特征具备更强的分类能力。特征的选取方法是：对训练数据集D，计算每一个特征的信息增益，选择最大的特征。

```
输入：训练数据集D和特征A
输出：特征A对训练数据集D的信息增益g(D, A)

计算数据集D的经验熵H(D)参考公式(7)
计算特征A对数据集D的经验条件熵H(D|A)参考公式(11)
计算信息增益 g(D, A) = H(D) - H(D|A)
```

在某数据集上分类困难时，即该数据集的经验熵很大的时候，根据公式(1)可知，此时的信息增益也会较大。比如说，当特征是连续的，这个特征将会把训练集划分成特别多的子集，最极端的情况是每个子集都只有一个样本，此时经验熵将变得很大， 而经验条件熵将会很小，于是信息增益将变得很大。如果单纯地按照信息增益来选去特征，此时的决策树将是一棵大却浅的树。因此提出**信息增益比**作为特征选择的另一准则，在信息增益的基础上除以经验熵,来**惩罚子集很多**的属性。

**定义2（信息增益比）：**特征$A$对数据集$D$的信息增益比$g_R(D,A)$定义为信息增益与经验熵的比值


$$
g_R(D, A)=\frac{g(D,A)}{H(D)}
$$


## 参考

**定义3（熵）：**设X是一个取有限个值的离散随机变量，其概率分布为


$$
P(X=x_i)=p_i，i=1,2,…,n
$$


则随机变量$X$的熵定义为


$$
H(X)=-\sum_{i=1}^{n}p_i \log p_i
$$


**定义4（条件熵）：**设有随机变量$(X, Y)$，其联合概率分布为


$$
P(X=x_i,Y=y_j)=p_{ij}，i=1,2,...n；j=1,2,...,m
$$


条件熵$H(Y \mid X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的**条件概率分布的熵**$H(Y \mid X=x_i)$对$X$的数学期望


$$
H(Y \mid X)=\sum_{i=1}^{n}p_i H(Y \mid X=x_i)
$$


**定义5（经验熵）：**设训练数据集为$D$，$\mid D \mid $表示其样本容量。数据集被分为$K$个类$C_k$，$\mid C_k \mid$表示属于类$C_k$的样本数量。估计某一数据属于类别$C_k$的概率分布为


$$
P(C_k)=\frac{|C_k|}{|D|}，k=1,2,...K
$$


则数据集$D​$的经验熵定义为


$$
H(D)=-\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log\frac{|C_k|}{|D|}
$$

**定义6（经验条件熵）：**设特征$A$有$n$个不同的取值${a_1,a_2,…,a_n}$，并将数据集$D$划分成$n$个子集$D_1,D_2,…,D_n$，$\mid D_i \mid$表示子集$D_i$的样本数量。令子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，$\mid D_{ik} \mid$为集合$D_{ik}$的样本数量。

于是某一数据属于子集$D_i$的概率分布为

$$
P(D_i)=\frac{|D_i|}{|D|}，i=1,2,...,n
$$


在某一数据属于子集$D_i$的条件下，又属于类$C_k$的条件概率分布为


$$
P(C_k \mid D_i)=\frac{|D_{ik}|}{|D_i|}
$$


于是某一数据属于子集$D_i$的条件下，又属于类$C_k$的**条件概率分布熵**为


$$
H(D_i)=-\sum_{k=1}^{K}P(C_k \mid D_i) \log P(C_k \mid D_i)=-\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|} \log \frac{|D_{ik}|}{|D_i|}
$$


则特征$A$对数据集$D$的经验条件熵为$H(D_i)$对$D_i$的数学期望：


$$
H(D \mid A)=\sum_{i=1}^{n}P(D_i)H(D_i)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}(\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|} \log \frac{|D_{ik}|}{|D_i|})
$$


## Reference

李航 《统计学习方法》第5章 决策树